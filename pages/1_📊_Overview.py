import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
import re
import ast

# ----------------------------------------
# PAGE CONFIG
# ----------------------------------------
st.set_page_config(
    page_title="Overview Dashboard",
    page_icon="ğŸ“Š",
    layout="wide"
)

st.title("ğŸ“Š Holistic Review Dashboard")
st.markdown("### ğŸ¦… ì „ì²´ ë¦¬ë·° í˜„í™© ë° ì¸í…”ë¦¬ì „ìŠ¤ ìš”ì•½")

# ========================================================
# LOAD DATA
# ========================================================
if "cluster_df" in st.session_state:
    df = st.session_state["cluster_df"].copy()
elif "clean_df" in st.session_state:
    df = st.session_state["clean_df"].copy()
else:
    st.error("âš  ë¨¼ì € Main í˜ì´ì§€ì—ì„œ ë¶„ì„ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.")
    st.stop()

# Preprocessing
if "at" in df.columns:
    df["at"] = pd.to_datetime(df["at"], errors="coerce")
    df = df.dropna(subset=["at"])

if "score" in df.columns:
    df["score"] = pd.to_numeric(df["score"], errors="coerce").fillna(0).astype(int)

# Map numeric 1-5 to Sentiment Group (Fallback)
def classify_sentiment_fallback(score):
    if score >= 4: return "Positive"
    elif score == 3: return "Neutral"
    else: return "Negative"

# Robust Semantic Version Parsing
def parse_version(v_str):
    """
    Parses version string into tuple (Major, Minor, Patch).
    Handles '1.2.3', 'v1.2', '1.2.3.4' etc.
    """
    if not isinstance(v_str, str): return (0, 0, 0)
    # Remove non-numeric prefixes/suffixes broadly
    clean_v = re.sub(r'[a-zA-Z_-]', '', v_str)
    # Find all number groups
    nums = [int(n) for n in re.findall(r'\d+', clean_v)]
    
    # Pad to at least 3 digits (Major, Minor, Patch)
    while len(nums) < 3:
        nums.append(0)
    return tuple(nums[:3])

# ----------------------------------------
# Robust Parsing Helper
# ----------------------------------------
def robust_eval_list(val):
    if pd.isna(val) or val == "":
        return []
    if isinstance(val, list):
        return val
    if isinstance(val, str):
        # Remove whitespace around brackets
        val = val.strip()
        # Handle simple cases or empty brackets
        if val == "[]": return []
        if val.startswith("[") and val.endswith("]"):
            try:
                return ast.literal_eval(val)
            except:
                pass
        # Fallback for comma-separated strings inside or outside brackets
        cleaned = val.replace("[", "").replace("]", "").replace("'", "").replace('"', "")
        return [x.strip() for x in cleaned.split(",") if x.strip()]
    return []

# Sentiment Labeling Logic (Robust)
if "sentiment" in df.columns:
    # Map Korean sentiment to English Label (Handle whitespace)
    def map_sentiment(s):
        s = str(s).strip()
        if s == "ê¸ì •": return "Positive"
        if s == "ë¶€ì •": return "Negative"
        return "Neutral"
    
    df["sentiment_label"] = df["sentiment"].apply(map_sentiment)
elif "sentiment_label" not in df.columns:
    df["sentiment_label"] = df["score"].apply(classify_sentiment_fallback)

# Ensure intensity is numeric
if "intensity" in df.columns:
    df["intensity"] = pd.to_numeric(df["intensity"], errors="coerce").fillna(1)
else:
    df["intensity"] = 1

# ========================================================
# 1. KPI CARDS
# ========================================================
# Helper for Numeric Sentiment (0-100)
def get_numeric_sentiment(label):
    if label == "Positive": return 100
    elif label == "Negative": return 0
    return 50

df["sentiment_score_val"] = df["sentiment_label"].apply(get_numeric_sentiment)

with st.container():
    c1, c2, c3 = st.columns(3)
    c1.metric("ì´ ë¦¬ë·° ìˆ˜", f"{len(df):,}")
    c2.metric("í‰ê·  í‰ì ", f"{df['score'].mean():.2f}/5")
    
    # Average Sentiment Score
    avg_senti = df["sentiment_score_val"].mean()
    c3.metric("í‰ê·  ê°ì •ì ìˆ˜", f"{avg_senti:.1f}/100", help="Positive(100), Neutral(50), Negative(0)")

st.markdown("---")

# ========================================================
# 0. CLUSTER IMPACT MATRIX (Issue Grouping)
# ========================================================
st.markdown("---")
st.subheader("0ï¸âƒ£ ì´ìŠˆ í´ëŸ¬ìŠ¤í„° ë¶„ì„ (Cluster Impact Matrix)")
st.caption("ê°œë³„ í‚¤ì›Œë“œê°€ ì•„ë‹Œ **ìœ ì‚¬í•œ ë¦¬ë·° ê·¸ë£¹(Cluster)** ë‹¨ìœ„ë¡œ ë¶„ì„í•˜ì—¬, ë” í° íë¦„ì„ íŒŒì•…í•©ë‹ˆë‹¤.")

# Use 'df' which implies cluster_df AND has sentiment_label calculated
if "cluster" in df.columns:
    c_df = df.copy()
    
    # Needs 'cluster' column
    # Filter out noise cluster usually labels as '-1' or empty
    c_df = c_df[c_df['cluster'].astype(str) != "-1"]
    
    # 1. Aggregation per Cluster
    # We need: Count, Avg Intensity, Main Sentiment, Representative Keywords
    
    # Helper for mode/top
    def get_top_item(series):
        try:
            vc = series.value_counts()
            if not vc.empty: return vc.index[0]
        except: pass
        return "Unknown"

    base_stats = c_df.groupby("cluster").agg(
        count=("reviewId", "count"),
        avg_intensity=("intensity", "mean"),
        avg_score=("score", "mean"),
        main_sentiment=("sentiment_label", get_top_item),
        main_category=("categories", get_top_item)
    ).reset_index()
    
    # Calculate Impact Score
    base_stats["impact_score"] = base_stats["count"] * base_stats["avg_intensity"]
    
    # Get Keywords
    cluster_kws = st.session_state.get("cluster_info", {})
    def get_cluster_label(cid):
        if cid in cluster_kws:
            return ", ".join(cluster_kws[cid][:3]) 
        return f"Cluster {cid}"
    base_stats["keywords_label"] = base_stats["cluster"].apply(get_cluster_label)

    # --------------------------------------------------------
    # 2. Relative Separation: Ensure we always have Neg/Pos
    # --------------------------------------------------------
    
    # --------------------------------------------------------
    # 2. Assign Group Type (Logic Update for N-/P- prefixes)
    # --------------------------------------------------------
    
    def determine_group_type(row):
        cid = str(row['cluster'])
        if cid.startswith("N-"): return "Negative (Risk)"
        if cid.startswith("P-"): return "Positive (Strength)"
        
        # Fallback: based on score
        if row['avg_score'] <= 3.2: return "Negative (Risk)"
        return "Positive (Strength)"
        
    base_stats["group_type"] = base_stats.apply(determine_group_type, axis=1)
    
    # 3. Take Top 5 Impact from Each Group
    final_neg = base_stats[base_stats["group_type"]=="Negative (Risk)"].sort_values("impact_score", ascending=False).head(5)
    final_pos = base_stats[base_stats["group_type"]=="Positive (Strength)"].sort_values("impact_score", ascending=False).head(5)
    
    # 4. Integrate for Visualization
    cluster_stats = pd.concat([final_neg, final_pos], ignore_index=True)
    
    if cluster_stats.empty:
        st.warning("í‘œì‹œí•  í´ëŸ¬ìŠ¤í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        # Scatter Plot using 'group_type' for consistent coloring
        fig_cls = px.scatter(
            cluster_stats,
            x="count",
            y="avg_intensity",
            size="impact_score",
            color="group_type", # Use the explicit group type
            text="keywords_label",
            hover_name="keywords_label",
            hover_data={"count":True, "avg_intensity":':.2f', "main_category":True, "cluster":True},
            labels={
                "count": "ë¦¬ë·° ìˆ˜ (Log Scale)", 
                "avg_intensity": "í‰ê·  ì‹¬ê°ë„/ê°•ë„ (1~5)",
                "group_type": "êµ¬ë¶„(Sentiment)",
                "keywords_label": "ëŒ€í‘œ í‚¤ì›Œë“œ"
            },
            title="Cluster Impact: Negative Risk vs Positive Strength",
            color_discrete_map={
                "Negative (Risk)": "#EF553B", 
                "Positive (Strength)": "#00CC96"
            },
            log_x=True,
            range_y=[1, 5.5]
        )
        
        # Improve Text Position so it doesn't overlap too much
        fig_cls.update_traces(textposition='top center')
    
        fig_cls.update_layout(height=600)
    
        st.plotly_chart(fig_cls, use_container_width=True)



# ========================================================
# 0. SENTIMENT BREAKDOWN (3-Column View)
# ========================================================
st.subheader("0ï¸âƒ£ ê°ì„±ë³„ í•µì‹¬ í‚¤ì›Œë“œ (Sentiment Breakdown)")
st.caption("ë¶€ì •(Risk), ì¤‘ë¦½(Feedback), ê¸ì •(Strength) ë¦¬ë·°ì—ì„œ ê°€ì¥ ë§ì´ ì–¸ê¸‰ëœ í‚¤ì›Œë“œë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.")

# Data Prep for Keywords
if "keywords" in df.columns:
    kw_df = df[["sentiment_label", "keywords"]].copy()
    kw_df["kw_list"] = kw_df["keywords"].apply(robust_eval_list)
    kw_df = kw_df.explode("kw_list")
    kw_df = kw_df[kw_df["kw_list"].notna()]
    kw_df = kw_df[kw_df["kw_list"] != ""]
    
    # [NORMALIZATION] Consolidate Synonyms & Handle Variations
    def normalize_kw_display(text):
        if not isinstance(text, str): return text
        text = text.strip()
        text_ns = text.replace(" ", "")
        
        # Manual Map (Consolidate to ID or Stop Target)
        map_dict = {
            "ì¬ë¯¸ìˆëŠ”": "ì¬ë¯¸", "ì¬ë°ŒëŠ”": "ì¬ë¯¸", "ê¿€ì¼": "ì¬ë¯¸", "ì¡´ì¼": "ì¬ë¯¸", "ì¼": "ì¬ë¯¸",
            "ê²Œì„í”Œë ˆì´": "ê²Œì„", "í”Œë ˆì´": "ê²Œì„", "Game": "ê²Œì„",
            "ì—…ëƒ": "ì—…ë°ì´íŠ¸", "íŒ¨ì¹˜": "ì—…ë°ì´íŠ¸", "ì—…ê·¸ë ˆì´ë“œ": "ì—…ë°ì´íŠ¸",
            "íƒ€ê²©": "íƒ€ê²©ê°",
            "ë™": "ìµœì í™”", "ë ‰": "ìµœì í™”", "íŠ•ê¹€": "ìµœì í™”", "ë°œì—´": "ìµœì í™”", "ë²„ë²…": "ìµœì í™”", "ëŠê¹€": "ìµœì í™”",
            "ìºë¦­": "ìºë¦­í„°", "ì—¬ìº": "ìºë¦­í„°", "ë‚¨ìº": "ìºë¦­í„°",
            "í˜„ì§ˆ": "ê³¼ê¸ˆ", "ê³¼ê¸ˆìœ ë„": "ê³¼ê¸ˆ",
            "ìš´ì˜ì": "ìš´ì˜", "ê°œë°œì": "ìš´ì˜",
            "ìŠ¤í† ë¦¬": "ìŠ¤í† ë¦¬", # Keep
            "ì•„íŠ¸": "ì•„íŠ¸/ê·¸ë˜í”½", "ê·¸ë˜í”½": "ì•„íŠ¸/ê·¸ë˜í”½", "ì¼ëŸ¬": "ì•„íŠ¸/ê·¸ë˜í”½", "ì¼ëŸ¬ìŠ¤íŠ¸": "ì•„íŠ¸/ê·¸ë˜í”½"
        }
        
        if text in map_dict: return map_dict[text]
        if text_ns in map_dict: return map_dict[text_ns]
        
        return text

    kw_df["kw_list"] = kw_df["kw_list"].apply(normalize_kw_display)

    # [FILTER] Remove generic/stop keywords from visualization
    STOP_KEYWORDS = {"ì¬ë¯¸", "ê²Œì„", "Good", "Play", "í•˜ëŠ”", "í• ", "í•¨", "ì „íˆ¬", "ìœ ì €", "ì‚¬ëŒ", "ê²ƒ", "ìˆ˜", "ì €", "ì œ"}
    kw_df = kw_df[~kw_df["kw_list"].isin(STOP_KEYWORDS)]
    
    # 2 Columns (Negative, Positive)
    col_neg, col_pos = st.columns(2)
    
    # Function to plot top keywords
    def plot_top_keywords(sent_filter, title, color_scale):
        subset = kw_df[kw_df["sentiment_label"] == sent_filter]
        if subset.empty:
            st.info(f"{title}: ë°ì´í„° ì—†ìŒ ({len(subset)}ê±´)")
            return
            
        top_k = subset["kw_list"].value_counts().head(10).reset_index()
        top_k.columns = ["keyword", "count"]
        
        if top_k.empty:
             st.info(f"{title}: í‚¤ì›Œë“œ ì—†ìŒ")
             return

        fig = px.bar(
            top_k,
            x="count",
            y="keyword",
            orientation='h',
            title=title,
            labels={"count": "ë¹ˆë„", "keyword": "í‚¤ì›Œë“œ"},
            color="count",
            color_continuous_scale=color_scale
        )
        fig.update_layout(yaxis={'categoryorder':'total ascending'}, showlegend=False, height=400)
        st.plotly_chart(fig, use_container_width=True)

    with col_neg:
        st.markdown("### ğŸ”´ ë¶€ì • (Negative)")
        plot_top_keywords("Negative", "Risk Keywords", "Reds")
        
    with col_pos:
        st.markdown("### ğŸ”µ ê¸ì • (Positive)")
        plot_top_keywords("Positive", "Strength Keywords", "Blues")

else:
    st.warning("í‚¤ì›Œë“œ ë°ì´í„°ê°€ ì—†ì–´ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

st.markdown("---")

st.markdown("---")

# ========================================================
# 1. HIERARCHICAL ANALYSIS (Treemap by Sentiment)
# ========================================================
st.subheader("1ï¸âƒ£ ê³„ì¸µí˜• ì´ìŠˆ ë¶„ì„ (Treemap by Sentiment)")
st.caption("ê°ì„±ë³„ë¡œ **ì£¼ì œ â†’ í‚¤ì›Œë“œ** ê³„ì¸µ êµ¬ì¡°ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤. (ë°•ìŠ¤ í¬ê¸° = ë¹ˆë„)")

# Prepare Data
# Prioritize 'refined_category' for consistent filtering if available
if "refined_category" in df.columns:
    df["categories"] = df["refined_category"]

# Prioritize 'refined_topic' (from cluster propagation)
possible_topics = ["refined_topic", "cluster_label", "categories", "category", "topic", "issue_summary"]
topic_col = next((c for c in possible_topics if c in df.columns), None)

# If no topic column, create a placeholder
if topic_col is None:
    df["topic_display"] = "General"
    topic_col = "topic_display"

# Clean Topic Column (Extract first item if list)
def clean_topic(val):
    if pd.isna(val): return "Etc"
    s_val = str(val).strip()
    
    # Empty cases
    if s_val in ["", "[]"]: return "Etc"
    
    # Check if it looks like a list string "['Topic']"
    if s_val.startswith("[") and s_val.endswith("]"):
        # Remove brackets and quotes to just get content
        cleaned = s_val.replace("[", "").replace("]", "").replace("'", "").replace('"', "")
        # Split by comma and take first item if exists
        items = [x.strip() for x in cleaned.split(",") if x.strip()]
        if items:
            return items[0]
        else:
            return "Etc"
            
    # Regular string (e.g. "ì„±ì¥") - Return as is
    return s_val

# Apply cleaning only if it looks like a list column (like categories) or ensure it's clean text
if topic_col in ["categories", "category", "topic", "refined_topic"]:
    df[topic_col] = df[topic_col].apply(clean_topic)

# Keyword Helper
def get_first_kw_robust(val):
    l = robust_eval_list(val)
    return l[0] if l else "Etc"
        
if "keywords" in df.columns:
    df["primary_keyword"] = df["keywords"].apply(get_first_kw_robust)
else:
    df["primary_keyword"] = "Unknown"

def plot_full_width_treemap(sent_label, color_scale, title):
    # Filter by Sentiment
    t_subset = df[df["sentiment_label"] == sent_label].copy()
    
    # Filter out Generic/Empty Topics & Keywords
    # We remove rows where Topic/Keyword is 'Etc', 'Unknown', 'None' etc.
    generic_terms = ["Etc", "Unknown", "None", "nan", ""]
    
    mask_valid_topic = ~t_subset[topic_col].astype(str).isin(generic_terms)
    # mask_valid_kw = ~t_subset["primary_keyword"].astype(str).isin(generic_terms)
    
    # Relaxed Filter: Show even if keyword is generic (user wants to see distribution)
    t_subset = t_subset[mask_valid_topic]

    if t_subset.empty:
        st.info(f"{title}: ìœ ì˜ë¯¸í•œ ë¶„ì„ ë°ì´í„°(Topic/Keyword)ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return

    tree_data = t_subset.groupby([topic_col, "primary_keyword"]).size().reset_index(name="count")
    # Filter out low frequency keywords (Keep > 2)
    tree_data = tree_data[tree_data["count"] > 2] # Filter <= 2 
    
    if tree_data.empty:
        st.info(f"{title}: í‘œì‹œí•  ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
        return

    # Treemap
    fig_tm = px.treemap(
        tree_data,
        path=[px.Constant(sent_label), topic_col, "primary_keyword"],
        values="count",
        color="count", # Color by magnitude to use the scale
        color_continuous_scale=color_scale,
        title=title,
        height=500  # Taller for better view
    )
    fig_tm.update_layout(margin=dict(t=30, l=10, r=10, b=10))
    st.plotly_chart(fig_tm, use_container_width=True)

# 1. Negative (Red)
st.markdown("### ğŸ”´ ë¶€ì • (Negative)")
plot_full_width_treemap("Negative", "Reds", "Negative Issues (Risk)")

# 3. Positive (Blue)
st.markdown("---")
st.markdown("### ğŸ”µ ê¸ì • (Positive)")
plot_full_width_treemap("Positive", "Blues", "Positive Strengths (Growth)")

# ========================================================
# 3. SENTIMENT TREND (Stacked Area)
# ========================================================
st.markdown("---")
st.subheader("3ï¸âƒ£ ê°ì„± íŠ¸ë Œë“œ ë³€í™” (Stacked Area)")
st.caption("ì‹œê°„ ë˜ëŠ” ë²„ì „ íë¦„ì— ë”°ë¥¸ ê¸/ë¶€ì • ë¦¬ë·° ë°œìƒëŸ‰ì˜ ë³€í™”ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")

# Control
trend_by = st.radio("ê¸°ì¤€ ì„ íƒ", ["ğŸ“… ì¼ë³„ (Date)", "ğŸ·ï¸ ë²„ì „ë³„ (Version)"], horizontal=True, index=0)

if "ì¼ë³„" in trend_by:
    x_col = "at"
    trend_df = df.set_index("at").groupby([pd.Grouper(freq="D"), "sentiment_label"]).size().unstack(fill_value=0).reset_index()
    x_title = "ë‚ ì§œ"
else:
    # Version-based: Sort by Release Date (Min 'at')
    x_col = "appVersion"
    if "appVersion" not in df.columns:
        st.error("ë°ì´í„°ì— 'appVersion' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
        
    # Calculate order (Semantic Version Sort)
    unique_versions = df["appVersion"].dropna().unique()
    try:
        # Try sorting by semantic versioning (Major.Minor.Patch)
        ver_order = sorted(unique_versions, key=parse_version)
    except:
        # Fallback to date min if parsing fails
        ver_order = df.groupby("appVersion")["at"].min().sort_values().index.tolist()
    
    trend_df = df.groupby(["appVersion", "sentiment_label"]).size().unstack(fill_value=0).reset_index()
    
    # Filter: Remove versions with <= 10 reviews to avoid distortion
    trend_df["Total_Count"] = trend_df[["Negative", "Neutral", "Positive"]].sum(axis=1)
    trend_df = trend_df[trend_df["Total_Count"] > 10]
    
    if trend_df.empty:
        st.warning("ë¦¬ë·° ìˆ˜ê°€ 10ê°œ ì´ˆê³¼ì¸ ë²„ì „ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # Sort
    trend_df["appVersion"] = pd.Categorical(trend_df["appVersion"], categories=ver_order, ordered=True)
    trend_df = trend_df.sort_values("appVersion")
    x_title = "ë²„ì „ (ë¦¬ë·° 10ê°œ ì´ˆê³¼)"

# Tabs
tab_vol, tab_ratio = st.tabs(["ğŸ“Š ë¦¬ë·° ìˆ˜ (Volume)", "ğŸ“ˆ ë¹„ìœ¨ (Ratio %)"])

with tab_vol:
    fig_area = px.area(
        trend_df,
        x=x_col,
        y=["Negative", "Neutral", "Positive"],
        color_discrete_map={
            "Positive": "#00CC96",
            "Neutral": "#AB63FA",
            "Negative": "#EF553B"
        },
        labels={"value": "ë¦¬ë·° ìˆ˜", x_col: x_title},
        title=f"{x_title}ë³„ ê°ì„± ë°œìƒëŸ‰ (ì ˆëŒ€ê°’)"
    )
    st.plotly_chart(fig_area, use_container_width=True)

with tab_ratio:
    # Calculate Average Sentiment Score Trend
    if x_col == "at": # Date
        s_trend = df.set_index("at").groupby(pd.Grouper(freq="D"))["sentiment_score_val"].mean().reset_index()
    else: # Version
        s_trend = df.groupby("appVersion")["sentiment_score_val"].mean().reset_index()
        # Sort version logic using 'ver_order' from 'Volume' block
        # We assume 'ver_order' exists if x_col != "at"
        s_trend["appVersion"] = pd.Categorical(s_trend["appVersion"], categories=ver_order, ordered=True)
        s_trend = s_trend.sort_values("appVersion")

    fig_line = px.line(
        s_trend,
        x=x_col,
        y="sentiment_score_val",
        labels={"sentiment_score_val": "í‰ê·  ê°ì •ì ìˆ˜", x_col: x_title},
        title=f"{x_title}ë³„ í‰ê·  ê°ì •ì ìˆ˜ ë³€í™” (Average Sentiment Score)"
    )
    fig_line.update_traces(line_color="#636EFA", mode="lines+markers")
    st.plotly_chart(fig_line, use_container_width=True)

# ========================================================
# 4. INTERACTIVE DATA TABLE
# ========================================================
st.markdown("---")
st.subheader("ğŸ” ì‹¬ì¸µ ë°ì´í„° íƒìƒ‰")

# Filters
f_col1, f_col2 = st.columns(2)
with f_col1:
    options_topic = sorted(df[topic_col].unique().astype(str))
    sel_topics = st.multiselect("ì£¼ì œ(Topic) í•„í„°", options=options_topic)
with f_col2:
    sel_senti = st.multiselect("ê°ì„±(Sentiment) í•„í„°", options=["Negative", "Neutral", "Positive"])

filtered_df = df.copy()
if sel_topics:
    # Ensure type match for filtering
    filtered_df = filtered_df[filtered_df[topic_col].astype(str).isin(sel_topics)]
if sel_senti:
    filtered_df = filtered_df[filtered_df["sentiment_label"].isin(sel_senti)]

# Sort by Thumbs Up Count
if "thumbsUpCount" in filtered_df.columns:
    filtered_df["thumbsUpCount"] = pd.to_numeric(filtered_df["thumbsUpCount"], errors="coerce").fillna(0).astype(int)
    filtered_df = filtered_df.sort_values("thumbsUpCount", ascending=False)
    
# Show Table with Clean Configuration
st.dataframe(
    filtered_df,
    column_order=["at", "thumbsUpCount", "score", "sentiment_label", topic_col, "primary_keyword", "content"],
    column_config={
        "at": st.column_config.DateColumn("ì‘ì„±ì¼", format="YYYY-MM-DD"),
        "thumbsUpCount": st.column_config.NumberColumn("ğŸ‘ ê³µê°", format="%d"),
        "score": st.column_config.NumberColumn("í‰ì ", format="%d â­"),
        "sentiment_label": "ê°ì„±",
        topic_col: "ì£¼ì œ",
        "primary_keyword": "í‚¤ì›Œë“œ",
        "content": st.column_config.TextColumn("ë¦¬ë·° ë‚´ìš©", width="large"),
    },
    use_container_width=True,
    height=400,
    hide_index=True
)
